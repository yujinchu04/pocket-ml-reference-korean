{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "추유진 - 2021데이터과학과머신러닝_수행평가.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yujinchu04/pocket-ml-reference-korean/blob/master/%EC%B6%94%EC%9C%A0%EC%A7%84_2021%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99%EA%B3%BC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%88%98%ED%96%89%ED%8F%89%EA%B0%80_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkpPmbzo3Bz7"
      },
      "source": [
        "# 2021 데이터과학과 머신러닝 수행평가\n",
        "* Direction : wineClass 데이터는 레드와인과 화이트와인, 그리고 와인질을 평가하는 데이터 셋이다.\n",
        "* 파일명은 wineClass.csv이다. \n",
        "* 다음을 딥러닝 모델로 학습시켜 가장 높은 정확도를 산출하시오.\n",
        "* 각 열의 이름은 다음과 같다.\n",
        "* 결합산,휘발산,구연산,잔여설탕,염화물,자유아황산가스,총황화합물,밀도,산성도,황화합물,알콜,와인질,와인종류\n",
        "* 이미 완성된 코드의 경우 수정하지 않고, 결과를 도출한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re3-aW6YU5oN"
      },
      "source": [
        "# 데이터 파일 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNK7h2Q9TZg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1b9f8f-9136-4e35-c56b-f89cd0d0fb10"
      },
      "source": [
        "import os\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# URL로 다운받은 파일 사용하기\n",
        "from urllib.request import urlretrieve\n",
        "urlretrieve(\"https://drive.google.com/uc?export=download&id=1rCT8Q3nHXpxA13qaSkGvnklG5gJmXxzM\", \"wineClass.csv\")\n",
        "###"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('wineClass.csv', <http.client.HTTPMessage at 0x7fb03376e150>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uplHMDgxnLnb"
      },
      "source": [
        "#라이브러리 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M51x0OEnN-z"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9ZfrsVmnTHB"
      },
      "source": [
        "# 데이터 셋 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II0mn_G_nPZJ"
      },
      "source": [
        "df=pd.read_csv('wineClass.csv',encoding='cp949')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SaX2macnbZo"
      },
      "source": [
        "# 데이터 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZyal_3SnbHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9212b40-e1da-4afe-8337-cd29ce88cebf"
      },
      "source": [
        "# 처음 5줄을 봅니다.\n",
        "print(df.head(50))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     결합산    휘발산   구연산  잔여설탕    염화물  ...   산성도  황화합물    알콜  와인질  와인종류\n",
            "0    7.4  0.700  0.00   1.9  0.076  ...  3.51  0.56   9.4    5     1\n",
            "1    7.8  0.880  0.00   2.6  0.098  ...  3.20  0.68   9.8    5     1\n",
            "2    7.8  0.760  0.04   2.3  0.092  ...  3.26  0.65   9.8    5     1\n",
            "3   11.2  0.280  0.56   1.9  0.075  ...  3.16  0.58   9.8    6     1\n",
            "4    7.4  0.700  0.00   1.9  0.076  ...  3.51  0.56   9.4    5     1\n",
            "5    7.4  0.660  0.00   1.8  0.075  ...  3.51  0.56   9.4    5     1\n",
            "6    7.9  0.600  0.06   1.6  0.069  ...  3.30  0.46   9.4    5     1\n",
            "7    7.3  0.650  0.00   1.2  0.065  ...  3.39  0.47  10.0    7     1\n",
            "8    7.8  0.580  0.02   2.0  0.073  ...  3.36  0.57   9.5    7     1\n",
            "9    7.5  0.500  0.36   6.1  0.071  ...  3.35  0.80  10.5    5     1\n",
            "10   6.7  0.580  0.08   1.8  0.097  ...  3.28  0.54   9.2    5     1\n",
            "11   7.5  0.500  0.36   6.1  0.071  ...  3.35  0.80  10.5    5     1\n",
            "12   5.6  0.615  0.00   1.6  0.089  ...  3.58  0.52   9.9    5     1\n",
            "13   7.8  0.610  0.29   1.6  0.114  ...  3.26  1.56   9.1    5     1\n",
            "14   8.9  0.620  0.18   3.8  0.176  ...  3.16  0.88   9.2    5     1\n",
            "15   8.9  0.620  0.19   3.9  0.170  ...  3.17  0.93   9.2    5     1\n",
            "16   8.5  0.280  0.56   1.8  0.092  ...  3.30  0.75  10.5    7     1\n",
            "17   8.1  0.560  0.28   1.7  0.368  ...  3.11  1.28   9.3    5     1\n",
            "18   7.4  0.590  0.08   4.4  0.086  ...  3.38  0.50   9.0    4     1\n",
            "19   7.9  0.320  0.51   1.8  0.341  ...  3.04  1.08   9.2    6     1\n",
            "20   8.9  0.220  0.48   1.8  0.077  ...  3.39  0.53   9.4    6     1\n",
            "21   7.6  0.390  0.31   2.3  0.082  ...  3.52  0.65   9.7    5     1\n",
            "22   7.9  0.430  0.21   1.6  0.106  ...  3.17  0.91   9.5    5     1\n",
            "23   8.5  0.490  0.11   2.3  0.084  ...  3.17  0.53   9.4    5     1\n",
            "24   6.9  0.400  0.14   2.4  0.085  ...  3.43  0.63   9.7    6     1\n",
            "25   6.3  0.390  0.16   1.4  0.080  ...  3.34  0.56   9.3    5     1\n",
            "26   7.6  0.410  0.24   1.8  0.080  ...  3.28  0.59   9.5    5     1\n",
            "27   7.9  0.430  0.21   1.6  0.106  ...  3.17  0.91   9.5    5     1\n",
            "28   7.1  0.710  0.00   1.9  0.080  ...  3.47  0.55   9.4    5     1\n",
            "29   7.8  0.645  0.00   2.0  0.082  ...  3.38  0.59   9.8    6     1\n",
            "30   6.7  0.675  0.07   2.4  0.089  ...  3.35  0.54  10.1    5     1\n",
            "31   6.9  0.685  0.00   2.5  0.105  ...  3.46  0.57  10.6    6     1\n",
            "32   8.3  0.655  0.12   2.3  0.083  ...  3.17  0.66   9.8    5     1\n",
            "33   6.9  0.605  0.12  10.7  0.073  ...  3.45  0.52   9.4    6     1\n",
            "34   5.2  0.320  0.25   1.8  0.103  ...  3.38  0.55   9.2    5     1\n",
            "35   7.8  0.645  0.00   5.5  0.086  ...  3.40  0.55   9.6    6     1\n",
            "36   7.8  0.600  0.14   2.4  0.086  ...  3.42  0.60  10.8    6     1\n",
            "37   8.1  0.380  0.28   2.1  0.066  ...  3.23  0.73   9.7    7     1\n",
            "38   5.7  1.130  0.09   1.5  0.172  ...  3.50  0.48   9.8    4     1\n",
            "39   7.3  0.450  0.36   5.9  0.074  ...  3.33  0.83  10.5    5     1\n",
            "40   7.3  0.450  0.36   5.9  0.074  ...  3.33  0.83  10.5    5     1\n",
            "41   8.8  0.610  0.30   2.8  0.088  ...  3.26  0.51   9.3    4     1\n",
            "42   7.5  0.490  0.20   2.6  0.332  ...  3.21  0.90  10.5    6     1\n",
            "43   8.1  0.660  0.22   2.2  0.069  ...  3.30  1.20  10.3    5     1\n",
            "44   6.8  0.670  0.02   1.8  0.050  ...  3.48  0.52   9.5    5     1\n",
            "45   4.6  0.520  0.15   2.1  0.054  ...  3.90  0.56  13.1    4     1\n",
            "46   7.7  0.935  0.43   2.2  0.114  ...  3.25  0.73   9.2    5     1\n",
            "47   8.7  0.290  0.52   1.6  0.113  ...  3.25  0.58   9.5    5     1\n",
            "48   6.4  0.400  0.23   1.6  0.066  ...  3.34  0.56   9.2    5     1\n",
            "49   5.6  0.310  0.37   1.4  0.074  ...  3.32  0.58   9.2    5     1\n",
            "\n",
            "[50 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTlf474nnlZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bccb123-b964-4a25-f2ab-095cd30fd4b1"
      },
      "source": [
        "# 데이터의 전반적인 정보를 확인해 봅니다.\n",
        "print(df.info())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6497 entries, 0 to 6496\n",
            "Data columns (total 13 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   결합산      6497 non-null   float64\n",
            " 1   휘발산      6497 non-null   float64\n",
            " 2   구연산      6497 non-null   float64\n",
            " 3   잔여설탕     6497 non-null   float64\n",
            " 4   염화물      6497 non-null   float64\n",
            " 5   자유아황산가스  6497 non-null   float64\n",
            " 6   총황화합물    6497 non-null   float64\n",
            " 7   밀도       6497 non-null   float64\n",
            " 8   산성도      6497 non-null   float64\n",
            " 9   황화합물     6497 non-null   float64\n",
            " 10  알콜       6497 non-null   float64\n",
            " 11  와인질      6497 non-null   int64  \n",
            " 12  와인종류     6497 non-null   int64  \n",
            "dtypes: float64(11), int64(2)\n",
            "memory usage: 660.0 KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNbTf9k3nncB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac38ac2d-604a-4e3b-b2c7-3646e7f5eb38"
      },
      "source": [
        "# 각 정보별 특징을 좀더 자세히 출력합니다.\n",
        "print(df.describe())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               결합산          휘발산  ...          와인질         와인종류\n",
            "count  6497.000000  6497.000000  ...  6497.000000  6497.000000\n",
            "mean      7.215307     0.339666  ...     5.818378     0.246114\n",
            "std       1.296434     0.164636  ...     0.873255     0.430779\n",
            "min       3.800000     0.080000  ...     3.000000     0.000000\n",
            "25%       6.400000     0.230000  ...     5.000000     0.000000\n",
            "50%       7.000000     0.290000  ...     6.000000     0.000000\n",
            "75%       7.700000     0.400000  ...     6.000000     0.000000\n",
            "max      15.900000     1.580000  ...     9.000000     1.000000\n",
            "\n",
            "[8 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8uXfgz2ofCn"
      },
      "source": [
        "#모델 설계"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEzK0fcIojnM"
      },
      "source": [
        "## 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oYkT4rgonjL"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WqVzSheoxY4"
      },
      "source": [
        "## 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMjJ0ERHAZDz",
        "outputId": "721ac995-1dc6-402e-f0fc-b154e31b185d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset=df.values\n",
        "X = dataset[:,0:12]\n",
        "Y = dataset[:,12]\n",
        "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3)\n",
        "#print(x_test)\n",
        "print(y_test)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtcDXkxIovaV"
      },
      "source": [
        "dataset = numpy.loadtxt(\"wineClass.csv\", delimiter=\",\", skiprows=1, encoding='cp949')\n",
        "X = dataset[:,0:12]\n",
        "Y = dataset[:,12]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7gj0aNDo5js"
      },
      "source": [
        "## 모델 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBRJ-4thoq-F"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=12, activation='relu')) #12, 12\n",
        "model.add(Dense(32, activation='relu')) #12, 24\n",
        "model.add(Dense(16, activation='relu')) #12, 12\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EswS6IrpBey"
      },
      "source": [
        "## 모델 컴파일"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfw-IcbKpBMm"
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jng1Ui-Tp1BB"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "             optimizer=\"adam\", #adam, RMSprop\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5D0hK4mpHg5"
      },
      "source": [
        "## 모델 학습시키기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5m6psFIpG9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb152423-12ab-4796-be2e-c0da3eac2ed0"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=200, batch_size=200)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "23/23 [==============================] - 1s 2ms/step - loss: 0.3033 - accuracy: 0.8571\n",
            "Epoch 2/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.9369\n",
            "Epoch 3/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2035 - accuracy: 0.9313\n",
            "Epoch 4/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9288\n",
            "Epoch 5/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9284\n",
            "Epoch 6/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1860 - accuracy: 0.9365\n",
            "Epoch 7/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1919 - accuracy: 0.9329\n",
            "Epoch 8/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1802 - accuracy: 0.9351\n",
            "Epoch 9/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1780 - accuracy: 0.9402\n",
            "Epoch 10/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1832 - accuracy: 0.9356\n",
            "Epoch 11/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9406\n",
            "Epoch 12/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1828 - accuracy: 0.9342\n",
            "Epoch 13/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1732 - accuracy: 0.9390\n",
            "Epoch 14/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1812 - accuracy: 0.9333\n",
            "Epoch 15/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1600 - accuracy: 0.9416\n",
            "Epoch 16/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1638 - accuracy: 0.9428\n",
            "Epoch 17/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1668 - accuracy: 0.9414\n",
            "Epoch 18/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1613 - accuracy: 0.9403\n",
            "Epoch 19/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1582 - accuracy: 0.9406\n",
            "Epoch 20/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1587 - accuracy: 0.9384\n",
            "Epoch 21/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.9448\n",
            "Epoch 22/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1587 - accuracy: 0.9414\n",
            "Epoch 23/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1478 - accuracy: 0.9453\n",
            "Epoch 24/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9462\n",
            "Epoch 25/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1406 - accuracy: 0.9477\n",
            "Epoch 26/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9466\n",
            "Epoch 27/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.9453\n",
            "Epoch 28/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 0.9475\n",
            "Epoch 29/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1443 - accuracy: 0.9423\n",
            "Epoch 30/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1469 - accuracy: 0.9435\n",
            "Epoch 31/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9505\n",
            "Epoch 32/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9435\n",
            "Epoch 33/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1302 - accuracy: 0.9519\n",
            "Epoch 34/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1280 - accuracy: 0.9500\n",
            "Epoch 35/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9536\n",
            "Epoch 36/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9522\n",
            "Epoch 37/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1230 - accuracy: 0.9542\n",
            "Epoch 38/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1230 - accuracy: 0.9504\n",
            "Epoch 39/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9599\n",
            "Epoch 40/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1244 - accuracy: 0.9557\n",
            "Epoch 41/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1139 - accuracy: 0.9569\n",
            "Epoch 42/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1221 - accuracy: 0.9534\n",
            "Epoch 43/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1109 - accuracy: 0.9582\n",
            "Epoch 44/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1148 - accuracy: 0.9556\n",
            "Epoch 45/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9586\n",
            "Epoch 46/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1093 - accuracy: 0.9607\n",
            "Epoch 47/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1086 - accuracy: 0.9606\n",
            "Epoch 48/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1075 - accuracy: 0.9612\n",
            "Epoch 49/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0986 - accuracy: 0.9673\n",
            "Epoch 50/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9644\n",
            "Epoch 51/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9678\n",
            "Epoch 52/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.9624\n",
            "Epoch 53/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9636\n",
            "Epoch 54/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9670\n",
            "Epoch 55/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9618\n",
            "Epoch 56/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1051 - accuracy: 0.9632\n",
            "Epoch 57/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1001 - accuracy: 0.9659\n",
            "Epoch 58/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.1012 - accuracy: 0.9644\n",
            "Epoch 59/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0921 - accuracy: 0.9678\n",
            "Epoch 60/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0895 - accuracy: 0.9668\n",
            "Epoch 61/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9679\n",
            "Epoch 62/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9719\n",
            "Epoch 63/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0869 - accuracy: 0.9693\n",
            "Epoch 64/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0946 - accuracy: 0.9676\n",
            "Epoch 65/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9713\n",
            "Epoch 66/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0852 - accuracy: 0.9723\n",
            "Epoch 67/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0838 - accuracy: 0.9737\n",
            "Epoch 68/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9728\n",
            "Epoch 69/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0933 - accuracy: 0.9723\n",
            "Epoch 70/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9704\n",
            "Epoch 71/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0791 - accuracy: 0.9763\n",
            "Epoch 72/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9745\n",
            "Epoch 73/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9730\n",
            "Epoch 74/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9778\n",
            "Epoch 75/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0859 - accuracy: 0.9737\n",
            "Epoch 76/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9734\n",
            "Epoch 77/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9748\n",
            "Epoch 78/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9766\n",
            "Epoch 79/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0717 - accuracy: 0.9783\n",
            "Epoch 80/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9782\n",
            "Epoch 81/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9764\n",
            "Epoch 82/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0720 - accuracy: 0.9783\n",
            "Epoch 83/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0805 - accuracy: 0.9754\n",
            "Epoch 84/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9789\n",
            "Epoch 85/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9776\n",
            "Epoch 86/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9822\n",
            "Epoch 87/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9775\n",
            "Epoch 88/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0674 - accuracy: 0.9799\n",
            "Epoch 89/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9798\n",
            "Epoch 90/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9773\n",
            "Epoch 91/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 0.9790\n",
            "Epoch 92/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0592 - accuracy: 0.9805\n",
            "Epoch 93/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0650 - accuracy: 0.9816\n",
            "Epoch 94/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9801\n",
            "Epoch 95/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0561 - accuracy: 0.9829\n",
            "Epoch 96/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0600 - accuracy: 0.9799\n",
            "Epoch 97/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0657 - accuracy: 0.9784\n",
            "Epoch 98/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9829\n",
            "Epoch 99/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0653 - accuracy: 0.9811\n",
            "Epoch 100/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9813\n",
            "Epoch 101/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9830\n",
            "Epoch 102/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9799\n",
            "Epoch 103/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0573 - accuracy: 0.9831\n",
            "Epoch 104/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9796\n",
            "Epoch 105/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0493 - accuracy: 0.9853\n",
            "Epoch 106/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0601 - accuracy: 0.9837\n",
            "Epoch 107/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9830\n",
            "Epoch 108/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9852\n",
            "Epoch 109/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0546 - accuracy: 0.9850\n",
            "Epoch 110/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0537 - accuracy: 0.9839\n",
            "Epoch 111/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9828\n",
            "Epoch 112/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0622 - accuracy: 0.9790\n",
            "Epoch 113/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9875\n",
            "Epoch 114/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9828\n",
            "Epoch 115/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0532 - accuracy: 0.9839\n",
            "Epoch 116/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9850\n",
            "Epoch 117/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9861\n",
            "Epoch 118/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9841\n",
            "Epoch 119/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9831\n",
            "Epoch 120/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9846\n",
            "Epoch 121/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9866\n",
            "Epoch 122/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9809\n",
            "Epoch 123/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0474 - accuracy: 0.9839\n",
            "Epoch 124/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9796\n",
            "Epoch 125/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9800\n",
            "Epoch 126/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0451 - accuracy: 0.9847\n",
            "Epoch 127/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9884\n",
            "Epoch 128/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 0.9863\n",
            "Epoch 129/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9867\n",
            "Epoch 130/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.9844\n",
            "Epoch 131/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0741 - accuracy: 0.9801\n",
            "Epoch 132/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9860\n",
            "Epoch 133/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9878\n",
            "Epoch 134/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9840\n",
            "Epoch 135/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9840\n",
            "Epoch 136/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0520 - accuracy: 0.9871\n",
            "Epoch 137/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9828\n",
            "Epoch 138/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0462 - accuracy: 0.9850\n",
            "Epoch 139/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0568 - accuracy: 0.9836\n",
            "Epoch 140/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0572 - accuracy: 0.9805\n",
            "Epoch 141/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9867\n",
            "Epoch 142/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9863\n",
            "Epoch 143/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0464 - accuracy: 0.9859\n",
            "Epoch 144/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9844\n",
            "Epoch 145/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0495 - accuracy: 0.9835\n",
            "Epoch 146/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0469 - accuracy: 0.9832\n",
            "Epoch 147/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 0.9871\n",
            "Epoch 148/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9857\n",
            "Epoch 149/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9862\n",
            "Epoch 150/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0474 - accuracy: 0.9860\n",
            "Epoch 151/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0545 - accuracy: 0.9837\n",
            "Epoch 152/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0607 - accuracy: 0.9841\n",
            "Epoch 153/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.9889\n",
            "Epoch 154/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0480 - accuracy: 0.9849\n",
            "Epoch 155/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9854\n",
            "Epoch 156/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0514 - accuracy: 0.9832\n",
            "Epoch 157/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0542 - accuracy: 0.9844\n",
            "Epoch 158/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0486 - accuracy: 0.9851\n",
            "Epoch 159/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9871\n",
            "Epoch 160/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9866\n",
            "Epoch 161/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0352 - accuracy: 0.9908\n",
            "Epoch 162/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0472 - accuracy: 0.9858\n",
            "Epoch 163/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0533 - accuracy: 0.9852\n",
            "Epoch 164/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0418 - accuracy: 0.9886\n",
            "Epoch 165/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0445 - accuracy: 0.9884\n",
            "Epoch 166/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0435 - accuracy: 0.9864\n",
            "Epoch 167/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9855\n",
            "Epoch 168/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.9850\n",
            "Epoch 169/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9853\n",
            "Epoch 170/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0453 - accuracy: 0.9852\n",
            "Epoch 171/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 0.9841\n",
            "Epoch 172/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9894\n",
            "Epoch 173/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9885\n",
            "Epoch 174/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0406 - accuracy: 0.9873\n",
            "Epoch 175/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9867\n",
            "Epoch 176/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9864\n",
            "Epoch 177/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0350 - accuracy: 0.9883\n",
            "Epoch 178/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0465 - accuracy: 0.9863\n",
            "Epoch 179/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0403 - accuracy: 0.9881\n",
            "Epoch 180/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0420 - accuracy: 0.9866\n",
            "Epoch 181/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9896\n",
            "Epoch 182/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9876\n",
            "Epoch 183/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 0.9876\n",
            "Epoch 184/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0396 - accuracy: 0.9865\n",
            "Epoch 185/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0384 - accuracy: 0.9899\n",
            "Epoch 186/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9876\n",
            "Epoch 187/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.9882\n",
            "Epoch 188/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0417 - accuracy: 0.9875\n",
            "Epoch 189/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9886\n",
            "Epoch 190/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0414 - accuracy: 0.9896\n",
            "Epoch 191/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0413 - accuracy: 0.9881\n",
            "Epoch 192/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0469 - accuracy: 0.9836\n",
            "Epoch 193/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9836\n",
            "Epoch 194/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9869\n",
            "Epoch 195/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0408 - accuracy: 0.9878\n",
            "Epoch 196/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0430 - accuracy: 0.9874\n",
            "Epoch 197/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9873\n",
            "Epoch 198/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0436 - accuracy: 0.9879\n",
            "Epoch 199/200\n",
            "23/23 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9848\n",
            "Epoch 200/200\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.0399 - accuracy: 0.9880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fafd4c30890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRz8TkjcpJ4x"
      },
      "source": [
        "## 결과 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBxvnUguoXw-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57451c31-6fc0-4b5a-b473-bbce71a4b03f"
      },
      "source": [
        "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61/61 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9810\n",
            "\n",
            " Test Accuracy: 0.9810\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "H02VKCCfvYIZ",
        "outputId": "378af1f3-ea2e-40a1-e9fc-bc68a96fbdd5"
      },
      "source": [
        "model.predict(7.4,0.7,0,1.9,\t0.076,\t11,\t34,\t0.9978,\t3.51,\t0.56,\t9.4)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-9f2ad9cb24c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7.4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.9\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;36m0.076\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;36m0.9978\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.51\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;36m0.56\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;36m9.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: predict() takes from 2 to 9 positional arguments but 12 were given"
          ]
        }
      ]
    }
  ]
}